{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents import ppo, a3c, cql, ddpg, dqn\n",
    "\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from source.envs.env import WhitedBasicModel\n",
    "from source.solvers.ray_solver import RaySolver\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "A3C_Trainer = a3c.A3CTrainer\n",
    "PPO_Trainer = ppo.PPOTrainer\n",
    "DQNTrainer = dqn.DQNTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'delta': 0.1, 'gamma': 1}\n",
      "{'delta': 0.1, 'gamma': 10}\n",
      "{'delta': 0.2, 'gamma': 1}\n",
      "{'delta': 0.2, 'gamma': 10}\n",
      "{'delta': 0.3, 'gamma': 1}\n",
      "{'delta': 0.3, 'gamma': 10}\n"
     ]
    }
   ],
   "source": [
    "from source.utils.useful_class import ParameterGrid\n",
    "\n",
    "grid = {\n",
    "        'delta': [0.1, 0.2, 0.3],\n",
    "        'gamma': [1, 10],\n",
    "    }\n",
    "pg = ParameterGrid(grid)\n",
    "for g in pg:\n",
    "    print(g)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-25 19:47:20,508\tINFO services.py:1340 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n",
      "2022-01-25 19:47:21,611\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-01-25 19:47:23,356\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
      "2022-01-25 19:47:23,378\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=18841)\u001B[0m 2022-01-25 19:47:23,323\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 0\n",
      "custom_metrics: {}\n",
      "date: 2022-01-25_19-47-23\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: e81217aa91364ff488289d6a917b607b\n",
      "hostname: mw-14.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      batch_count: 10\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0001\n",
      "        entropy_coeff: 0.01\n",
      "        grad_gnorm: 168.23855590820312\n",
      "        policy_entropy: 29.95711326599121\n",
      "        policy_loss: -94.7829818725586\n",
      "        vf_loss: 54.918212890625\n",
      "  num_steps_sampled: 10\n",
      "  num_steps_trained: 10\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 8\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.5\n",
      "  ram_util_percent: 89.4\n",
      "pid: 18812\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 0.09767699241638184\n",
      "time_this_iter_s: 0.09767699241638184\n",
      "time_total_s: 0.09767699241638184\n",
      "timers:\n",
      "  apply_grad_throughput: 760.637\n",
      "  apply_grad_time_ms: 13.147\n",
      "  grad_wait_time_ms: 81.417\n",
      "  update_time_ms: 0.982\n",
      "timestamp: 1643161643\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 10\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 0\n",
      "custom_metrics: {}\n",
      "date: 2022-01-25_19-47-28\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 289.0444147651193\n",
      "episode_reward_mean: -4035077171.258187\n",
      "episode_reward_min: -539981025956.9159\n",
      "episodes_this_iter: 168\n",
      "episodes_total: 168\n",
      "experiment_id: e81217aa91364ff488289d6a917b607b\n",
      "hostname: mw-14.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      batch_count: 10\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0001\n",
      "        entropy_coeff: 0.01\n",
      "        grad_gnorm: 667.2374267578125\n",
      "        policy_entropy: 23.34882354736328\n",
      "        policy_loss: 192.6551971435547\n",
      "        vf_loss: 336.1049499511719\n",
      "  num_steps_sampled: 16880\n",
      "  num_steps_trained: 16880\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 2\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 8\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 90.58571428571429\n",
      "  ram_util_percent: 89.77142857142857\n",
      "pid: 18812\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03917921575157504\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08566893442802348\n",
      "  mean_inference_ms: 1.1119123208023258\n",
      "  mean_raw_obs_processing_ms: 0.1204908160578378\n",
      "time_since_restore: 5.095010042190552\n",
      "time_this_iter_s: 4.99733304977417\n",
      "time_total_s: 5.095010042190552\n",
      "timers:\n",
      "  apply_grad_throughput: 11929.532\n",
      "  apply_grad_time_ms: 0.838\n",
      "  grad_wait_time_ms: 1.084\n",
      "  update_time_ms: 0.726\n",
      "timestamp: 1643161648\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 16880\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 0\n",
      "custom_metrics: {}\n",
      "date: 2022-01-25_19-47-33\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 275.5754564997233\n",
      "episode_reward_mean: -133.62563823075257\n",
      "episode_reward_min: -31258.538132555623\n",
      "episodes_this_iter: 184\n",
      "episodes_total: 352\n",
      "experiment_id: e81217aa91364ff488289d6a917b607b\n",
      "hostname: mw-14.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      batch_count: 10\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0001\n",
      "        entropy_coeff: 0.01\n",
      "        grad_gnorm: 496.8706970214844\n",
      "        policy_entropy: 21.918575286865234\n",
      "        policy_loss: 120.43544006347656\n",
      "        vf_loss: 227.09471130371094\n",
      "  num_steps_sampled: 35070\n",
      "  num_steps_trained: 35070\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 3\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 8\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 91.72857142857144\n",
      "  ram_util_percent: 89.64285714285715\n",
      "pid: 18812\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03839500921385817\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08433816189092967\n",
      "  mean_inference_ms: 1.0815048494452093\n",
      "  mean_raw_obs_processing_ms: 0.1145674452806363\n",
      "time_since_restore: 10.083951234817505\n",
      "time_this_iter_s: 4.988941192626953\n",
      "time_total_s: 10.083951234817505\n",
      "timers:\n",
      "  apply_grad_throughput: 12100.234\n",
      "  apply_grad_time_ms: 0.826\n",
      "  grad_wait_time_ms: 1.043\n",
      "  update_time_ms: 0.751\n",
      "timestamp: 1643161653\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 35070\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 0\n",
      "custom_metrics: {}\n",
      "date: 2022-01-25_19-47-38\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 299.45644342501134\n",
      "episode_reward_mean: -202.38887754992834\n",
      "episode_reward_min: -51307.07961391157\n",
      "episodes_this_iter: 184\n",
      "episodes_total: 536\n",
      "experiment_id: e81217aa91364ff488289d6a917b607b\n",
      "hostname: mw-14.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      batch_count: 10\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0001\n",
      "        entropy_coeff: 0.01\n",
      "        grad_gnorm: 226.51148986816406\n",
      "        policy_entropy: 16.076730728149414\n",
      "        policy_loss: 52.12196350097656\n",
      "        vf_loss: 20.97702980041504\n",
      "  num_steps_sampled: 53160\n",
      "  num_steps_trained: 53160\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 4\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 8\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 88.64285714285714\n",
      "  ram_util_percent: 89.87142857142858\n",
      "pid: 18812\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03821873723240375\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08396369165098194\n",
      "  mean_inference_ms: 1.0741004129218334\n",
      "  mean_raw_obs_processing_ms: 0.11276707114365457\n",
      "time_since_restore: 15.073577165603638\n",
      "time_this_iter_s: 4.989625930786133\n",
      "time_total_s: 15.073577165603638\n",
      "timers:\n",
      "  apply_grad_throughput: 10655.719\n",
      "  apply_grad_time_ms: 0.938\n",
      "  grad_wait_time_ms: 1.189\n",
      "  update_time_ms: 0.853\n",
      "timestamp: 1643161658\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 53160\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 0\n",
      "custom_metrics: {}\n",
      "date: 2022-01-25_19-47-43\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 229.21936119940818\n",
      "episode_reward_mean: 127.94503569778553\n",
      "episode_reward_min: -1273.9670393694319\n",
      "episodes_this_iter: 170\n",
      "episodes_total: 706\n",
      "experiment_id: e81217aa91364ff488289d6a917b607b\n",
      "hostname: mw-14.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      batch_count: 10\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0001\n",
      "        entropy_coeff: 0.01\n",
      "        grad_gnorm: 212.47120666503906\n",
      "        policy_entropy: 14.89747142791748\n",
      "        policy_loss: 41.98146057128906\n",
      "        vf_loss: 33.961769104003906\n",
      "  num_steps_sampled: 70600\n",
      "  num_steps_trained: 70600\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 5\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 8\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 76.77142857142859\n",
      "  ram_util_percent: 89.74285714285715\n",
      "pid: 18812\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03835635985422894\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0841756591921185\n",
      "  mean_inference_ms: 1.077256934149747\n",
      "  mean_raw_obs_processing_ms: 0.11253022465356094\n",
      "time_since_restore: 20.054532051086426\n",
      "time_this_iter_s: 4.980954885482788\n",
      "time_total_s: 20.054532051086426\n",
      "timers:\n",
      "  apply_grad_throughput: 11904.138\n",
      "  apply_grad_time_ms: 0.84\n",
      "  grad_wait_time_ms: 0.908\n",
      "  update_time_ms: 0.73\n",
      "timestamp: 1643161663\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 70600\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 0\n",
      "custom_metrics: {}\n",
      "date: 2022-01-25_19-47-48\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 344.62285343989413\n",
      "episode_reward_mean: 168.3027589625828\n",
      "episode_reward_min: -715.7471828705628\n",
      "episodes_this_iter: 190\n",
      "episodes_total: 896\n",
      "experiment_id: e81217aa91364ff488289d6a917b607b\n",
      "hostname: mw-14.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      batch_count: 10\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0001\n",
      "        entropy_coeff: 0.01\n",
      "        grad_gnorm: 54.078800201416016\n",
      "        policy_entropy: 10.305500984191895\n",
      "        policy_loss: 10.335261344909668\n",
      "        vf_loss: 1.8299455642700195\n",
      "  num_steps_sampled: 89660\n",
      "  num_steps_trained: 89660\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 6\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 8\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.97142857142856\n",
      "  ram_util_percent: 89.49999999999999\n",
      "pid: 18812\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.038105333478883105\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08354199374866607\n",
      "  mean_inference_ms: 1.0679154024872464\n",
      "  mean_raw_obs_processing_ms: 0.11135279080292881\n",
      "time_since_restore: 25.04355812072754\n",
      "time_this_iter_s: 4.989026069641113\n",
      "time_total_s: 25.04355812072754\n",
      "timers:\n",
      "  apply_grad_throughput: 11324.938\n",
      "  apply_grad_time_ms: 0.883\n",
      "  grad_wait_time_ms: 1.053\n",
      "  update_time_ms: 0.764\n",
      "timestamp: 1643161668\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 89660\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 0\n",
      "custom_metrics: {}\n",
      "date: 2022-01-25_19-47-53\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 303.97241787704974\n",
      "episode_reward_mean: 172.8512866046336\n",
      "episode_reward_min: -143.24803699762884\n",
      "episodes_this_iter: 184\n",
      "episodes_total: 1080\n",
      "experiment_id: e81217aa91364ff488289d6a917b607b\n",
      "hostname: mw-14.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      batch_count: 10\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0001\n",
      "        entropy_coeff: 0.01\n",
      "        grad_gnorm: 96.26947784423828\n",
      "        policy_entropy: 8.157395362854004\n",
      "        policy_loss: 1.6487269401550293\n",
      "        vf_loss: 20.87812614440918\n",
      "  num_steps_sampled: 107810\n",
      "  num_steps_trained: 107810\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 7\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 8\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 90.01428571428572\n",
      "  ram_util_percent: 89.4\n",
      "pid: 18812\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.038122994115006074\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08346578810053659\n",
      "  mean_inference_ms: 1.06766654502125\n",
      "  mean_raw_obs_processing_ms: 0.11117827287227575\n",
      "time_since_restore: 30.032050132751465\n",
      "time_this_iter_s: 4.988492012023926\n",
      "time_total_s: 30.032050132751465\n",
      "timers:\n",
      "  apply_grad_throughput: 12861.229\n",
      "  apply_grad_time_ms: 0.778\n",
      "  grad_wait_time_ms: 0.985\n",
      "  update_time_ms: 0.774\n",
      "timestamp: 1643161673\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 107810\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 0\n",
      "custom_metrics: {}\n",
      "date: 2022-01-25_19-47-58\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 315.8612682064778\n",
      "episode_reward_mean: 180.7164299111773\n",
      "episode_reward_min: 93.79166113241149\n",
      "episodes_this_iter: 178\n",
      "episodes_total: 1258\n",
      "experiment_id: e81217aa91364ff488289d6a917b607b\n",
      "hostname: mw-14.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      batch_count: 10\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0001\n",
      "        entropy_coeff: 0.01\n",
      "        grad_gnorm: 146.61831665039062\n",
      "        policy_entropy: 5.903464317321777\n",
      "        policy_loss: 8.29516887664795\n",
      "        vf_loss: 12.686630249023438\n",
      "  num_steps_sampled: 125820\n",
      "  num_steps_trained: 125820\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 8\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 8\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 76.72857142857143\n",
      "  ram_util_percent: 89.57142857142857\n",
      "pid: 18812\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.03814524026591694\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08355063101372581\n",
      "  mean_inference_ms: 1.0679556810282338\n",
      "  mean_raw_obs_processing_ms: 0.11113852208080552\n",
      "time_since_restore: 35.023585081100464\n",
      "time_this_iter_s: 4.991534948348999\n",
      "time_total_s: 35.023585081100464\n",
      "timers:\n",
      "  apply_grad_throughput: 10999.145\n",
      "  apply_grad_time_ms: 0.909\n",
      "  grad_wait_time_ms: 1.19\n",
      "  update_time_ms: 1.002\n",
      "timestamp: 1643161678\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 125820\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 0\n",
      "custom_metrics: {}\n",
      "date: 2022-01-25_19-48-03\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 348.17048201658235\n",
      "episode_reward_mean: 178.56258883711033\n",
      "episode_reward_min: -41.87571642219306\n",
      "episodes_this_iter: 190\n",
      "episodes_total: 1448\n",
      "experiment_id: e81217aa91364ff488289d6a917b607b\n",
      "hostname: mw-14.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      batch_count: 10\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0001\n",
      "        entropy_coeff: 0.01\n",
      "        grad_gnorm: 287.124267578125\n",
      "        policy_entropy: 7.149305820465088\n",
      "        policy_loss: 20.461612701416016\n",
      "        vf_loss: 67.88206481933594\n",
      "  num_steps_sampled: 144380\n",
      "  num_steps_trained: 144380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 9\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 8\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 77.5625\n",
      "  ram_util_percent: 89.36250000000001\n",
      "pid: 18812\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0380473564005493\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08334446321646367\n",
      "  mean_inference_ms: 1.0648588560920251\n",
      "  mean_raw_obs_processing_ms: 0.1107098146707388\n",
      "time_since_restore: 40.01425290107727\n",
      "time_this_iter_s: 4.990667819976807\n",
      "time_total_s: 40.01425290107727\n",
      "timers:\n",
      "  apply_grad_throughput: 12613.689\n",
      "  apply_grad_time_ms: 0.793\n",
      "  grad_wait_time_ms: 0.986\n",
      "  update_time_ms: 0.755\n",
      "timestamp: 1643161683\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 144380\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 0\n",
      "custom_metrics: {}\n",
      "date: 2022-01-25_19-48-08\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 311.1779371433593\n",
      "episode_reward_mean: 155.06735418098168\n",
      "episode_reward_min: -1670.744244750364\n",
      "episodes_this_iter: 184\n",
      "episodes_total: 1632\n",
      "experiment_id: e81217aa91364ff488289d6a917b607b\n",
      "hostname: mw-14.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      batch_count: 10\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0001\n",
      "        entropy_coeff: 0.01\n",
      "        grad_gnorm: 791.685791015625\n",
      "        policy_entropy: 7.568018913269043\n",
      "        policy_loss: 61.47217559814453\n",
      "        vf_loss: 393.6155090332031\n",
      "  num_steps_sampled: 162800\n",
      "  num_steps_trained: 162800\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 10\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 8\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 88.25714285714287\n",
      "  ram_util_percent: 89.32857142857142\n",
      "pid: 18812\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.038021234818987906\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.08330575348203208\n",
      "  mean_inference_ms: 1.0636256038841687\n",
      "  mean_raw_obs_processing_ms: 0.11058461378065892\n",
      "time_since_restore: 45.004058837890625\n",
      "time_this_iter_s: 4.9898059368133545\n",
      "time_total_s: 45.004058837890625\n",
      "timers:\n",
      "  apply_grad_throughput: 11755.666\n",
      "  apply_grad_time_ms: 0.851\n",
      "  grad_wait_time_ms: 0.955\n",
      "  update_time_ms: 0.834\n",
      "timestamp: 1643161688\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 162800\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "\n",
      "checkpoint saved at /Users/mingweima/ray_results/A3C_my-env_2022-01-25_19-47-2158sfyiph/checkpoint_000010/checkpoint-10\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "env = WhitedBasicModel(env_config={\"structural_params\": {\"gamma\": [0.9,0.96],\n",
    "                                                         \"delta\": [0.1, 0.3],\n",
    "                                                         \"theta\": [0.5, 0.8],\n",
    "                                                         \"rho\": [0.3, 0.8],\n",
    "                                                         \"sigma\": [0., 0.15],\n",
    "                                                        }, \n",
    "                                   \"env_params\": {\"psi_func\": lambda i, k: 0.01*i**2/(2*k)\n",
    "                                                 },\n",
    "                                   \"is_mutable\": True,\n",
    "                                  })\n",
    "solver = RaySolver(env=env,\n",
    "                   trainer=A3C_Trainer,\n",
    "                   solver_params={\"verbose\": True, \"episodes\": 10,\n",
    "                                  \"trainer_config\": {\n",
    "                                      \"num_workers\": 8,\n",
    "                                      \"gamma\": env.current_structural_params.get(\"gamma\", 0.99),\n",
    "                                  }\n",
    "                                  })\n",
    "solver.train()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# solver.trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solver.trainer.restore('/Users/mingweima/ray_results/A3C_my-env_2022-01-25_18-51-04pgozg8qt/checkpoint_000010/checkpoint-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = sample_obs = solver.sample(param_dict={\"gamma\": 0.9,\n",
    "                             \"delta\": 0.15,\n",
    "                             \"theta\": 0.5,\n",
    "                             \"rho\": 0.5,\n",
    "                             \"sigma\": 0.15,\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33575694.91361878"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_params = \n",
    "\n",
    "(data=data,  # (nsamples, N, T) or (N, T); N: obs dim, T: eps length\n",
    "                 solver=solver,\n",
    "                 estimator_params: ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "cp = []\n",
    "for eps in range(2):\n",
    "      # instantiate env class\n",
    "      episode_reward = 0\n",
    "      done = False\n",
    "      obs = env.reset()\n",
    "      # run until episode ends\n",
    "      caps = []\n",
    "      while not done:\n",
    "          action = solver.trainer.compute_single_action(obs, clip_action=True)\n",
    "          obs, reward, done, info = env.step(action, resample_param=False)\n",
    "          episode_reward += reward\n",
    "          #print(action, obs, reward, done)\n",
    "          caps += [obs[0]]\n",
    "      cp += [ [caps] ]\n",
    "cp = np.squeeze(np.array(cp)).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}